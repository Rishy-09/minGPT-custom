# ğŸ”¤ SpellCorrect-GPT: A Tiny GPT for Spelling Error Correction

A decoder-only Transformer model trained from scratch on 1M+ noisy-to-clean sentence pairs to learn **character-level spelling correction** using attention.

Built using a custom version of [minGPT](https://github.com/karpathy/minGPT) and trained on a Kaggle dataset of real spelling mistakes.

---

## ğŸ“‚ Project Structure

```bash
minGPT-custom/
â”œâ”€â”€ mingpt/                # All core modules (model, attention, etc.)
â”‚   â”œâ”€â”€ model.py           # GPT model
â”‚   â”œâ”€â”€ trainer.py         # Training loop abstraction
â”‚   â”œâ”€â”€ utils.py           # Seed setting, sampling, etc.
â”‚   â””â”€â”€ bpe.py             # Custom BPE tokenizer
|-- projects/spellcorrectgpt/
|     |__ train.py         # â¬…ï¸ Run this to train the model
|     |__sample.py         # â¬…ï¸ Run this to test spell correction
â”œâ”€â”€spellcorrectgpt_final.pt      # pameters (weights of different layers) is stored in it
â””â”€â”€ README.md              # This file
```

---

## ğŸš€ Core Idea

Train a lightweight GPT to correct spelling mistakes using only character-level BPE tokens â€” no need for massive language understanding. Focus is on:

- Learning patterns of common human typos  
- Fixing messy sentences using attention mechanisms  
- Running with small compute on local/Kaggle  

---

## ğŸ“Š Dataset

**Name:** `spelling-mistake-data-1mn`  
**Source:** [Kaggle Dataset](https://www.kaggle.com/datasets/samarthagarwal23/spelling-mistake-data-1mn)  
**Size:** 1M+ sentence pairs (Noisy input â†’ Corrected output)

![Dataset](data1.jpeg)

---

## ğŸ§  Model Details

- Decoder-only GPT (Karpathy-style)
- Trained from scratch
- Tokenized using custom BPE tokenizer
- Sequence length: 128
- Positional embeddings: learned
- Model size: ~12.34M parameters

![Weights](weights1.jpeg)
---

## ğŸ› ï¸ How to Run

### â–¶ï¸ Train the model

**Run**:
```bash
python train.py
```

**Note**: Edit paths in `config.py` or `train.py` if you're using Kaggle or Colab.

- On Kaggle, upload files and set `input.txt`, and model paths accordingly.
- On local, ensure dataset is placed correctly and adjust `os.path.join()` logic in `train.py`.

---

### ğŸ” Test / Inference

**Run**:
```bash
python sample.py
```

It loads the `spellcorrectgpt_final.pt` weights and tokenizes input for correction.

![test](test1.jpeg)
---

## ğŸ“¦ Outputs

- `spellcorrectgpt_final.pt` â€” Trained PyTorch checkpoint (~12M params)

![train](train1.jpeg)
---

## ğŸ“š Credits

- Architecture: Adapted from [Karpathyâ€™s minGPT](https://github.com/karpathy/minGPT)
- Dataset: Kaggle spelling correction set
- Tokenization: Custom BPE tokenizer
- Training/Testing: Custom `train.py` and `sample.py` scripts

---

## ğŸ”— Related Repos

- [minGPT](https://github.com/karpathy/minGPT)
- [NanoGPT](https://github.com/karpathy/nanoGPT) â€” For scaled-up training later

---

## ğŸ§ª Future Ideas

- Add multi-language support  
- Train on noisy web data (social media text)
- Integrate with text editors or chat UIs

---

> Made with ğŸ”¥ and attention heads.
